<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Call for Papers – D1 @ UbiComp 2025</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-02950907ecc0f8bdf1de2c53f3eb8625.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">D1 @ UbiComp 2025</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./cfp.html" aria-current="page"> 
<span class="menu-text">Call for Papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./organizers.html"> 
<span class="menu-text">Organizers</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#important-dates" id="toc-important-dates" class="nav-link active" data-scroll-target="#important-dates">Important dates</a></li>
  <li><a href="#important-links" id="toc-important-links" class="nav-link" data-scroll-target="#important-links">Important links</a></li>
  <li><a href="#the-dataset" id="toc-the-dataset" class="nav-link" data-scroll-target="#the-dataset">The dataset</a></li>
  <li><a href="#paper-submission" id="toc-paper-submission" class="nav-link" data-scroll-target="#paper-submission">Paper submission</a></li>
  <li><a href="#paper-presentation" id="toc-paper-presentation" class="nav-link" data-scroll-target="#paper-presentation">Paper presentation</a></li>
  <li><a href="#paper-awards" id="toc-paper-awards" class="nav-link" data-scroll-target="#paper-awards">Paper Awards</a></li>
  <li><a href="#special-track-in-journal" id="toc-special-track-in-journal" class="nav-link" data-scroll-target="#special-track-in-journal">Special track in Journal</a></li>
  <li><a href="#how-to-get-the-datasets" id="toc-how-to-get-the-datasets" class="nav-link" data-scroll-target="#how-to-get-the-datasets">How to get the datasets</a></li>
  <li><a href="#contacts" id="toc-contacts" class="nav-link" data-scroll-target="#contacts">Contacts</a></li>
  <li><a href="#examples-of-previous-studies" id="toc-examples-of-previous-studies" class="nav-link" data-scroll-target="#examples-of-previous-studies">Examples of previous studies</a>
  <ul class="collapse">
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Call for Papers</h1>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<p>The open challenge aims to explore the <em>DiversityOne</em> dataset, one of the largest and most geographically diverse smartphone datasets about everyday life behavior. The dataset combines questionnaires about demographic and psychosocial variables from 18K participants, and passive smartphone sensor data and self-reported annotation from 782 students across eight universities in eight countries. This dataset is a rich and valuable research resource that can be used to address research questions in multiple fields: machine learning, mobile sensing, computational social science, design with data, and many others. The open challenge offers the opportunity to work on the dataset and gain useful feedback on your research. We welcome contributions from researchers from <em>diverse backgrounds and geographical provenances</em>. In particular, we welcome contributions that address aspects including, but not limited to:</p>
<ul>
<li><p><strong>AI/ubiquitous computing/mobile sensing</strong></p>
<ul>
<li>data-centric AI</li>
<li>interactive machine learning</li>
<li>noisy annotation detection and correction</li>
<li>domain adaptation</li>
<li>transfer learning</li>
<li>activity and mood recognition</li>
<li>responsible and ethical AI</li>
</ul></li>
<li><p><strong>Computational social science</strong></p>
<ul>
<li>network analysis of social systems</li>
<li>sequence analysis of diary data</li>
<li>analysis of communities of practices</li>
<li>machine learning or rule-based analysis of social behavior</li>
</ul></li>
<li><p><strong>Designing with data</strong></p>
<ul>
<li>studies focusing on the design and documentation of the dataset collection</li>
<li>studies focusing on the design affordances of the dataset</li>
<li>data-centric design</li>
<li>user-centered design</li>
</ul></li>
</ul>
<section id="important-dates" class="level2">
<h2 class="anchored" data-anchor-id="important-dates">Important dates</h2>
<ul>
<li><p><strong>From March 28, 2025</strong>: Submit your research proposal using the web form (<em>the link will be added on this website from March 28</em>) and request the datasets that you need to answer your research questions. Please specify that you are requesting the dataset to submit your work to the workshop. The full list of available datasets and documentation is accessible on the <a href="https://datascientiafoundation.github.io/LivePeople-ws/datasets/">data catalog</a>.</p></li>
<li><p><strong>June 8, 2025:</strong> Abstract deadline. Link to submission platform <em>TBA</em></p></li>
<li><p><strong>June 15, 2025</strong>: Submission deadline</p></li>
<li><p><strong>June 29, 2025</strong>: Author notification</p></li>
<li><p><strong>July 30, 2025</strong>: Deadline for the camera-ready version of workshop papers to be included in the ACM DL</p></li>
<li><p><strong>October 12 or 13, 2015</strong>: Full-day Workshop.</p></li>
</ul>
</section>
<section id="important-links" class="level2">
<h2 class="anchored" data-anchor-id="important-links">Important links</h2>
<p><strong>Dataset paper</strong> <a href="https://dl.acm.org/doi/10.1145/3712289">https://dl.acm.org/doi/10.1145/3712289</a></p>
<p><strong>Data catalog</strong> <a href="https://datascientiafoundation.github.io/LivePeople-ws/datasets/">https://datascientiafoundation.github.io/LivePeople-ws/datasets/</a></p>
<p><strong>Dataset webpage</strong> <a href="https://datascientia.eu/projects/diversityone/">https://datascientia.eu/projects/diversityone/</a></p>
<p><strong>Conference webpage UbiComp / ISWC 2025</strong> <a href="https://www.ubicomp.org/ubicomp-iswc-2025/">https://www.ubicomp.org/ubicomp-iswc-2025/</a></p>
</section>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">The dataset</h2>
<p>To participate, you are expected to use the <em>DiversityOne</em> dataset. This dataset is the result of the large-scale European project <a href="https://doi.org/10.3030/823783">“WeNet - The Internet of US”</a>. The <a href="https://dl.acm.org/doi/10.1145/3712289">dataset paper</a> describes in detail the dataset together with the data collection methodology and statistics. contains data from college students in eight countries: China, Denmark, India, Italy, Mexico, Mongolia, Paraguay, and the United Kingdom. The study followed ethical approval procedures in each of the participating institutions and is compliant with the European General Data Protection Regulation (GDPR). The dataset contains questionnaire answers from over 18K students, 782 of whom agreed to participate in a longitudinal survey of four weeks. The study used the <a href="https://datascientia.disi.unitn.it/ilog/">iLog app</a> to collect data from smartphone sensors such as accelerometer, gyroscope, and GPS, as well as derived information such as notification interactions, app usage, activities, and step counts. During this period, participants reported their activities, locations, social context, and mood, with daily reports on sleep quality and daily expectations. The combination of sensor data and self-reported data across eight universities worldwide fosters research in ubiquitous computing, mobile sensing, machine learning, computational social science, and design with data. The dataset also opens the possibility for the design of new data-driven studies of diverse human behavior.</p>
</section>
<section id="paper-submission" class="level2">
<h2 class="anchored" data-anchor-id="paper-submission">Paper submission</h2>
<p><em>Short paper (max 4 pages, excluding references)</em>. The submitted works are expected to reflect on, analyze, or test the <em>DiversityOne</em> dataset. Submitted papers should report the work’s motivation, methodology, results, possible future analyses, and include an ethical statement describing potential societal impact. The paper should follow the <a href="https://www.ubicomp.org/ubicomp-iswc-2025/authors/formatting/">UbiComp/ISWC template and guideline</a>. All papers and any supplementary material must be <strong>anonymized</strong>. Each submission will be reviewed by two reviewers from a panel of experts. All authors are asked to adhere to the <a href="https://sigchi.org/resources/guides-for-authors/accessibility/">Accessible Submission Guidelines</a>. All accepted publications will be published on the ACM Digital Library as part of the UbiComp 2025 proceedings. At least one author of each accepted paper needs to register for the workshop. During the workshop, each paper will be presented by one of the authors. We are exploring with the conference and workshop chairs the possibility of supporting video/remote presentations and reduced rates for authors unable to travel to the conference to promote diversity and facilitate participation.</p>
</section>
<section id="paper-presentation" class="level2">
<h2 class="anchored" data-anchor-id="paper-presentation">Paper presentation</h2>
<p>The authors of the accepted paper will be invited to present their work during the workshop. The presentation timing will be defined based on the number of submissions (tentatively, the presentation will be between 10 minutes, with 5 minutes for Q/A and discussion, in which we encourage participants to exchange ideas and approaches). The final timing will be communicated before the workshop.</p>
</section>
<section id="paper-awards" class="level2">
<h2 class="anchored" data-anchor-id="paper-awards">Paper Awards</h2>
<p>Papers will receive a special mention during the workshop in the form of a best paper award and a best paper runner-up award. The evaluation will be made by a committee based on the following criteria:</p>
<ul>
<li><em>creativity</em>: how original or novel the analysis is;</li>
<li><em>multidisciplinary</em>: how well it combines ideas and approaches from multiple disciplines;</li>
<li><em>presentation</em>: how clear the presentation is and how well written the paper is;</li>
<li><em>impact</em>: how likely is it that the work can lead to impactful results if the paper is further extended.</li>
</ul>
</section>
<section id="special-track-in-journal" class="level2">
<h2 class="anchored" data-anchor-id="special-track-in-journal">Special track in Journal</h2>
<p>A selection of the accepted papers will be invited to submit an extended version to a special track in a journal related to the topics of the workshop (details will be released in the next few weeks).</p>
</section>
<section id="how-to-get-the-datasets" class="level2">
<h2 class="anchored" data-anchor-id="how-to-get-the-datasets">How to get the datasets</h2>
<ol type="1">
<li><p><strong>Navigate the data catalog.</strong> The catalog is available at <a href="https://datascientiafoundation.github.io/LivePeople-ws/datasets/" class="uri">https://datascientiafoundation.github.io/LivePeople-ws/datasets/</a>. Please look at the available datasets and documentation. The catalog contains both basic datasets (e.g., one sensor) and bundles of basic sensors (e.g., motion sensors such as accelerometer and step counter) split by data collection location. Any composition of bundle, datasets and data collection location can be requested coherently with your research proposal.</p></li>
<li><p><strong>Proposal submission.</strong> All authors must submit a data download request through the web form that will be made available from <em>March 28, 2025</em> by providing the following information:</p>
<ul>
<li>Names and contact information of the authors working on the dataset and their institution.</li>
<li>Description of the research proposal. Provide a description of your idea and how you plan to use the dataset.</li>
<li>Select from a list the datasets to download. On the data catalog, you can find and navigate the list of downloadable datasets and navigate the codebooks.</li>
</ul>
<p>The key requirements to obtain a copy of the data are the affiliation with a research institution, either private or public, the coherence between the requested data and the research proposal, and the acceptance of the Terms and License Agreement. Key licensing terms include:</p>
<ul>
<li>datasets are used exclusively for research purposes;</li>
<li>redistribution of the datasets is prohibited;</li>
<li>datasets cannot be publicly shared (e.g., on a website);</li>
<li>any attempt to reverse engineer any portion of the data or to re-identify the participants is strictly forbidden and could constitute unlawful processing of personal data.</li>
</ul></li>
<li><p><strong>Dataset download.</strong> The proposal request is evaluated by the University of Trento (UNITN), and in case of a positive response, the participants receive an email with the instructions for downloading the dataset after a few days. The requested data are shared with the participants through dedicated storage.</p></li>
</ol>
</section>
<section id="contacts" class="level2">
<h2 class="anchored" data-anchor-id="contacts">Contacts</h2>
<p>For any questions related to the workshop or technical support regarding the datasets, please do not hesitate to reach out to the organizers at <strong>datadistribution.knowdive [at] unitn.it</strong>.</p>
</section>
<section id="examples-of-previous-studies" class="level2">
<h2 class="anchored" data-anchor-id="examples-of-previous-studies">Examples of previous studies</h2>
<p>Previous studies investigated various aspects of high-quality rich datasets that include sensor data and self-reports. Some examples are: the use of social media <span class="citation" data-cites="giunchiglia2018mobile">(<a href="#ref-giunchiglia2018mobile" role="doc-biblioref">Giunchiglia et al. 2018</a>)</span>, the quality of answers and mislabeling <span class="citation" data-cites="bontempelli2020learning">(<a href="#ref-bontempelli2020learning" role="doc-biblioref">Bontempelli et al. 2020</a>)</span>, the usefulness of self-reports towards understanding the user’s subjective perspective of the local context <span class="citation" data-cites="zhang2021putting">(<a href="#ref-zhang2021putting" role="doc-biblioref">Zhang et al. 2021</a>)</span>, the impact of COVID on the students’ lives <span class="citation" data-cites="girardini2023adaptation">(<a href="#ref-girardini2023adaptation" role="doc-biblioref">Girardini et al. 2023</a>)</span>, cross-individual activity recognition <span class="citation" data-cites="shen2022federated">(<a href="#ref-shen2022federated" role="doc-biblioref">Shen et al. 2022</a>)</span>, mood inference <span class="citation" data-cites="meegahapola2023generalization">(<a href="#ref-meegahapola2023generalization" role="doc-biblioref">Meegahapola et al. 2023</a>)</span>, diversity perceptions in a community <span class="citation" data-cites="kun2022">(<a href="#ref-kun2022" role="doc-biblioref">Kun et al. 2022</a>)</span>, activity recognition <span class="citation" data-cites="bouton2022your">(<a href="#ref-bouton2022your" role="doc-biblioref">Bouton-Bessac, Meegahapola, and Gatica-Perez 2022</a>)</span>, social context inference while eating <span class="citation" data-cites="kammoun2023understanding">(<a href="#ref-kammoun2023understanding" role="doc-biblioref">Kammoun, Meegahapola, and Gatica-Perez 2023</a>)</span>, inferring mood-while-eating <span class="citation" data-cites="bangamuarachchi2023inferring">(<a href="#ref-bangamuarachchi2023inferring" role="doc-biblioref">Bangamuarachchi et al. 2025</a>)</span>, and the generation of contextually rich data with other reference datasets <span class="citation" data-cites="giunchiglia2024big">(<a href="#ref-giunchiglia2024big" role="doc-biblioref">Giunchiglia and Li 2024</a>)</span>.</p>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bangamuarachchi2023inferring" class="csl-entry" role="listitem">
Bangamuarachchi, Wageesha, Anju Chamantha, Lakmal Meegahapola, Haeeun Kim, Salvador Ruiz-Correa, Indika Perera, and Daniel Gatica-Perez. 2025. <span>“Inferring Mood-While-Eating with Smartphone Sensing and Community-Based Model Personalization.”</span> <em>ACM Transactions on Computing for Healthcare (HEALTH)</em>. <a href="https://arxiv.org/abs/2306.00723">https://arxiv.org/abs/2306.00723</a>.
</div>
<div id="ref-bontempelli2020learning" class="csl-entry" role="listitem">
Bontempelli, Andrea, Stefano Teso, Fausto Giunchiglia, and Andrea Passerini. 2020. <span>“Learning in the Wild with Incremental Skeptical Gaussian Processes.”</span> In <em>IJCAI</em>. <a href="https://www.ijcai.org/proceedings/2020/0399.pdf">https://www.ijcai.org/proceedings/2020/0399.pdf</a>.
</div>
<div id="ref-bouton2022your" class="csl-entry" role="listitem">
Bouton-Bessac, Emma, Lakmal Meegahapola, and Daniel Gatica-Perez. 2022. <span>“Your Day in Your Pocket: Complex Activity Recognition from Smartphone Accelerometers.”</span> In <em>International Conference on Pervasive Computing Technologies for Healthcare</em>, 247–58. Springer. <a href="https://www.idiap.ch/\~gatica/publications/BoutonBessacEtAl-ph22.pdf">https://www.idiap.ch/\~gatica/publications/BoutonBessacEtAl-ph22.pdf</a>.
</div>
<div id="ref-girardini2023adaptation" class="csl-entry" role="listitem">
Girardini, Nicolò Alessandro, Simone Centellegher, Andrea Passerini, Ivano Bison, Fausto Giunchiglia, and Bruno Lepri. 2023. <span>“Adaptation of Student Behavioural Routines During Covid-19: A Multimodal Approach.”</span> <em>EPJ Data Science</em> 12 (1): 55. <a href="https://epjds.epj.org/articles/epjdata/abs/2023/01/13688_2023_Article_429/13688_2023_Article_429.html">https://epjds.epj.org/articles/epjdata/abs/2023/01/13688_2023_Article_429/13688_2023_Article_429.html</a>.
</div>
<div id="ref-giunchiglia2024big" class="csl-entry" role="listitem">
Giunchiglia, Fausto, and Xiaoyue Li. 2024. <span>“Big-Thick Data Generation via Reference and Personal Context Unification.”</span> In <em>ECAI 2024</em>, 1975–84. IOS Press. <a href="https://ebooks.iospress.nl/doi/10.3233/FAIA240713">https://ebooks.iospress.nl/doi/10.3233/FAIA240713</a>.
</div>
<div id="ref-giunchiglia2018mobile" class="csl-entry" role="listitem">
Giunchiglia, Fausto, Mattia Zeni, Elisa Gobbi, Enrico Bignotti, and Ivano Bison. 2018. <span>“Mobile Social Media Usage and Academic Performance.”</span> <em>Computers in Human Behavior</em> 82: 177–85. <a href="https://arxiv.org/abs/2004.01392">https://arxiv.org/abs/2004.01392</a>.
</div>
<div id="ref-kammoun2023understanding" class="csl-entry" role="listitem">
Kammoun, Nathan, Lakmal Meegahapola, and Daniel Gatica-Perez. 2023. <span>“Understanding the Social Context of Eating with Multimodal Smartphone Sensing: The Role of Country Diversity.”</span> In <em>Proceedings of the 25th International Conference on Multimodal Interaction</em>, 604–12. <a href="https://arxiv.org/abs/2306.00709">https://arxiv.org/abs/2306.00709</a>.
</div>
<div id="ref-kun2022" class="csl-entry" role="listitem">
Kun, Peter, Amalia de Götzen, Miriam Bidoglia, Niels Jørgen Gommesen, and George Gaskell. 2022. <span>“Exploring Diversity Perceptions in a Community Through a q&amp;a Chatbot.”</span> In <em>DRS2022: Bilbao Design Research Society</em>, 1–19. <a href="https://arxiv.org/abs/2402.08558">https://arxiv.org/abs/2402.08558</a>.
</div>
<div id="ref-meegahapola2023generalization" class="csl-entry" role="listitem">
Meegahapola, Lakmal, William Droz, Peter Kun, Amalia De Götzen, Chaitanya Nutakki, Shyam Diwakar, et al. 2023. <span>“Generalization and Personalization of Mobile Sensing-Based Mood Inference Models: An Analysis of College Students in Eight Countries.”</span> <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 6 (4): 1–32. <a href="https://arxiv.org/abs/2211.03009">https://arxiv.org/abs/2211.03009</a>.
</div>
<div id="ref-shen2022federated" class="csl-entry" role="listitem">
Shen, Qiang, Haotian Feng, Rui Song, Stefano Teso, Fausto Giunchiglia, Hao Xu, et al. 2022. <span>“Federated Multi-Task Attention for Cross-Individual Human Activity Recognition.”</span> In <em>IJCAI</em>, 3423–29. IJCAI. <a href="https://www.ijcai.org/proceedings/2022/0475.pdf">https://www.ijcai.org/proceedings/2022/0475.pdf</a>.
</div>
<div id="ref-zhang2021putting" class="csl-entry" role="listitem">
Zhang, Wanyi, Qiang Shen, Stefano Teso, Bruno Lepri, Andrea Passerini, Ivano Bison, and Fausto Giunchiglia. 2021. <span>“Putting Human Behavior Predictability in Context.”</span> <em>EPJ Data Science</em> 10 (1): 42. <a href="https://epjds.epj.org/articles/epjdata/abs/2021/01/13688_2021_Article_299/13688_2021_Article_299.html">https://epjds.epj.org/articles/epjdata/abs/2021/01/13688_2021_Article_299/13688_2021_Article_299.html</a>.
</div>
</div>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>